{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04d57595",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chengxin/anaconda3/envs/vagen/lib/python3.10/site-packages/lightning_fabric/__init__.py:29: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __import__(\"pkg_resources\").declare_namespace(__name__)\n",
      "/home/chengxin/anaconda3/envs/vagen/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/chengxin/chengxin/vssflow/feature/extract_rawnet.py:22: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"ffmpeg\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports completed!\n"
     ]
    }
   ],
   "source": [
    "# VAFlow Inference Script\n",
    "# 输入: video, transcript, reference audio\n",
    "# 输出: 生成的 audio 和 speech\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "import sys\n",
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn.functional as nn_func\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import math\n",
    "\n",
    "\n",
    "sys.path.insert(0, '/home/chengxin/chengxin/vssflow')\n",
    "sys.path.insert(0, '/home/chengxin/chengxin/vssflow/feature')\n",
    "\n",
    "# 导入必要的模块\n",
    "from worker.vaflow_noise_lip_synch_text import VAFlow, WrappedModel\n",
    "from flow_matching.solver import ODESolver\n",
    "from diffusers.models.embeddings import get_1d_rotary_pos_embed\n",
    "from diffusers.utils.torch_utils import randn_tensor\n",
    "from feature.extract_phoneme import TextCleaner, PhonemeTokenizer\n",
    "from feature.extract_rawnet import extract_speaker_embd\n",
    "from feature.extract_clip import get_video_frames, encode_image\n",
    "from feature.extract_synchformer import VideoDataset, encode_video_with_sync\n",
    "from model.clip.clip_module import CLIPViT\n",
    "from RawNet.python.RawNet3.models.RawNet3 import RawNet3\n",
    "from RawNet.python.RawNet3.models.RawNetBasicBlock import Bottle2neck\n",
    "import soundfile as sf\n",
    "from einops import rearrange\n",
    "from synchformer import Synchformer\n",
    "\n",
    "print(\"Imports completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd8f3ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded. Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ========== 配置参数 ==========\n",
    "# 模型配置\n",
    "CONFIG = {\n",
    "    # 模型路径\n",
    "    \"ckpt_dir_vae\":\"./assets/vae\",\n",
    "    \"ckpt_dir_image_encoder\": \"./assets/clip/ViT-B-16.pt\",\n",
    "    \"ckpt_dir_audio_dit\": \"./assets/stable_audio/ckpt/transformer_lip_synch_text\",\n",
    "    \"ckpt_dir_vocoder\": \"./assets/vocoder\",\n",
    "    \"vaflow_ckpt_path\": \"./log/2025_12_28-22_22_24-vaflow_noise_lip_synch_text_v2c_synthesized/ckpt/epoch=0199-step=6.00e+03.ckpt\",  # 修改为你的checkpoint路径\n",
    "    \"rawnet_ckpt_path\": \"./feature/RawNet/python/RawNet3/models/weights/model.pt\",\n",
    "    \"synchformer_ckpt_path\": \"./assets/synchformer/synchformer_state_dict.pth\",\n",
    "    \"token_list_path\": \"./data/token_list.json\",  # phoneme到id的映射文件\n",
    "    \n",
    "    # 模型参数\n",
    "    \"phone_ebd_dim\": 32,\n",
    "    \"cond_feat_dim\": 768,\n",
    "    \"lip_feat_dim\": 1024,\n",
    "    \"synch_feat_dim\": 768,\n",
    "    \"dit_num_layers\": 10,\n",
    "    \"original_channel\": 128,\n",
    "    \"audio_duration_sec\": 10,\n",
    "    \"audio_length_per_sec\": 25,\n",
    "    \"vae_latent_scaling_factor\": 1.0,\n",
    "    \"scale_factor\": 1.0,\n",
    "    \n",
    "    # 推理参数\n",
    "    \"guidance_scale\": 3.0,\n",
    "    \"sample_steps\": 10,\n",
    "    \"sample_method\": \"dopri5\",\n",
    "    \"audio_sample_rate\": 16000,\n",
    "    \"num_samples_per_prompt\": 1,\n",
    "    \n",
    "    # 设备\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "}\n",
    "\n",
    "# 视频处理配置\n",
    "VIDEO_PROCESS_CONFIG = {\n",
    "    'duration': 10.0,\n",
    "    'resize_input_size': [224, 224],\n",
    "    'target_sampling_rate': 10,\n",
    "    'raw_duration_min_threshold': 0.05\n",
    "}\n",
    "\n",
    "print(f\"Configuration loaded. Device: {CONFIG['device']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac68d1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./assets/stable_audio/ckpt/transformer_lip_synch_text were not used when initializing StableAudioDiTModel: ['transformer_blocks.20.attn2.to_out.0.weight', 'transformer_blocks.23.norm2.bias', 'transformer_blocks.13.norm3.bias', 'transformer_blocks.23.attn2.to_v.weight', 'transformer_blocks.15.ff.net.0.proj.weight', 'transformer_blocks.16.ff.net.0.proj.weight', 'transformer_blocks.18.norm3.weight', 'transformer_blocks.14.ff.net.2.bias', 'transformer_blocks.10.ff.net.2.weight', 'transformer_blocks.15.norm1.bias', 'transformer_blocks.20.norm2.bias', 'transformer_blocks.11.norm1.bias', 'transformer_blocks.15.norm3.bias', 'transformer_blocks.20.attn2.to_q.weight', 'transformer_blocks.13.attn2.to_k.weight', 'transformer_blocks.17.ff.net.0.proj.bias', 'transformer_blocks.20.attn1.to_out.0.weight', 'transformer_blocks.14.attn1.to_out.0.weight', 'transformer_blocks.12.ff.net.2.bias', 'transformer_blocks.17.norm1.weight', 'transformer_blocks.16.attn2.to_v.weight', 'transformer_blocks.10.attn1.to_q.weight', 'transformer_blocks.18.norm3.bias', 'transformer_blocks.14.norm3.weight', 'transformer_blocks.20.ff.net.0.proj.bias', 'transformer_blocks.21.attn1.to_q.weight', 'transformer_blocks.22.attn2.to_out.0.weight', 'transformer_blocks.13.attn1.to_q.weight', 'transformer_blocks.10.norm2.bias', 'transformer_blocks.13.norm1.bias', 'transformer_blocks.17.attn2.to_v.weight', 'transformer_blocks.23.attn1.to_q.weight', 'transformer_blocks.10.attn1.to_v.weight', 'transformer_blocks.12.norm1.weight', 'transformer_blocks.23.ff.net.2.weight', 'transformer_blocks.13.attn2.to_out.0.weight', 'transformer_blocks.22.attn2.to_k.weight', 'transformer_blocks.19.attn2.to_out.0.weight', 'transformer_blocks.14.norm3.bias', 'transformer_blocks.15.attn2.to_out.0.weight', 'transformer_blocks.14.norm1.weight', 'transformer_blocks.17.ff.net.2.weight', 'transformer_blocks.22.norm1.bias', 'transformer_blocks.23.norm2.weight', 'transformer_blocks.18.attn2.to_v.weight', 'transformer_blocks.21.attn2.to_v.weight', 'transformer_blocks.20.attn2.to_k.weight', 'transformer_blocks.16.norm2.bias', 'transformer_blocks.19.attn2.to_q.weight', 'transformer_blocks.19.attn1.to_q.weight', 'transformer_blocks.11.attn1.to_out.0.weight', 'transformer_blocks.11.ff.net.0.proj.weight', 'transformer_blocks.13.attn1.to_v.weight', 'transformer_blocks.21.attn2.to_q.weight', 'transformer_blocks.23.norm1.weight', 'transformer_blocks.20.norm2.weight', 'transformer_blocks.20.norm3.weight', 'transformer_blocks.19.attn1.to_k.weight', 'transformer_blocks.18.attn1.to_v.weight', 'transformer_blocks.12.norm3.bias', 'transformer_blocks.22.norm3.bias', 'transformer_blocks.16.attn2.to_q.weight', 'transformer_blocks.11.norm1.weight', 'transformer_blocks.20.norm1.weight', 'transformer_blocks.14.attn1.to_v.weight', 'transformer_blocks.15.ff.net.2.weight', 'transformer_blocks.23.attn2.to_q.weight', 'transformer_blocks.12.norm2.weight', 'transformer_blocks.10.attn2.to_out.0.weight', 'transformer_blocks.17.attn1.to_k.weight', 'transformer_blocks.15.norm1.weight', 'transformer_blocks.22.attn2.to_v.weight', 'transformer_blocks.15.norm3.weight', 'transformer_blocks.20.attn1.to_q.weight', 'transformer_blocks.21.attn2.to_k.weight', 'transformer_blocks.15.ff.net.0.proj.bias', 'transformer_blocks.19.norm1.weight', 'transformer_blocks.11.ff.net.2.bias', 'transformer_blocks.18.ff.net.2.bias', 'transformer_blocks.17.norm3.weight', 'transformer_blocks.12.attn2.to_out.0.weight', 'transformer_blocks.21.attn1.to_v.weight', 'transformer_blocks.22.norm1.weight', 'transformer_blocks.23.attn2.to_k.weight', 'transformer_blocks.23.attn1.to_out.0.weight', 'transformer_blocks.14.attn2.to_k.weight', 'transformer_blocks.12.norm1.bias', 'transformer_blocks.22.ff.net.0.proj.weight', 'transformer_blocks.14.attn2.to_q.weight', 'transformer_blocks.12.norm3.weight', 'transformer_blocks.16.norm2.weight', 'transformer_blocks.15.attn2.to_v.weight', 'transformer_blocks.15.attn1.to_v.weight', 'transformer_blocks.21.norm1.weight', 'transformer_blocks.12.attn1.to_k.weight', 'transformer_blocks.19.ff.net.2.weight', 'transformer_blocks.19.ff.net.0.proj.weight', 'transformer_blocks.10.attn2.to_v.weight', 'transformer_blocks.22.ff.net.0.proj.bias', 'transformer_blocks.13.ff.net.2.weight', 'transformer_blocks.13.norm2.bias', 'transformer_blocks.13.attn2.to_v.weight', 'transformer_blocks.20.attn2.to_v.weight', 'transformer_blocks.14.attn2.to_v.weight', 'transformer_blocks.19.attn1.to_v.weight', 'transformer_blocks.16.attn1.to_q.weight', 'transformer_blocks.12.attn1.to_q.weight', 'transformer_blocks.21.attn2.to_out.0.weight', 'transformer_blocks.14.attn1.to_k.weight', 'transformer_blocks.23.ff.net.0.proj.weight', 'transformer_blocks.21.norm1.bias', 'transformer_blocks.15.norm2.bias', 'transformer_blocks.20.ff.net.0.proj.weight', 'transformer_blocks.12.attn2.to_q.weight', 'transformer_blocks.11.ff.net.2.weight', 'transformer_blocks.19.attn2.to_k.weight', 'transformer_blocks.17.attn1.to_q.weight', 'transformer_blocks.14.norm2.weight', 'transformer_blocks.19.ff.net.0.proj.bias', 'transformer_blocks.22.norm2.weight', 'cross_attention_proj.2.weight', 'transformer_blocks.20.norm1.bias', 'transformer_blocks.12.attn1.to_out.0.weight', 'transformer_blocks.22.attn1.to_out.0.weight', 'transformer_blocks.16.attn1.to_k.weight', 'transformer_blocks.19.ff.net.2.bias', 'transformer_blocks.17.norm2.weight', 'transformer_blocks.10.norm3.weight', 'transformer_blocks.10.norm1.weight', 'transformer_blocks.14.norm2.bias', 'transformer_blocks.23.attn1.to_k.weight', 'transformer_blocks.11.attn2.to_k.weight', 'transformer_blocks.18.attn1.to_q.weight', 'transformer_blocks.19.norm2.bias', 'transformer_blocks.13.attn1.to_k.weight', 'transformer_blocks.12.attn1.to_v.weight', 'transformer_blocks.15.ff.net.2.bias', 'transformer_blocks.10.norm1.bias', 'transformer_blocks.22.attn1.to_q.weight', 'transformer_blocks.17.attn1.to_out.0.weight', 'transformer_blocks.22.norm2.bias', 'transformer_blocks.16.norm1.bias', 'transformer_blocks.14.ff.net.0.proj.bias', 'transformer_blocks.11.attn2.to_out.0.weight', 'transformer_blocks.14.ff.net.2.weight', 'transformer_blocks.13.norm2.weight', 'transformer_blocks.18.ff.net.2.weight', 'transformer_blocks.11.norm2.bias', 'transformer_blocks.17.ff.net.0.proj.weight', 'transformer_blocks.23.ff.net.0.proj.bias', 'transformer_blocks.20.ff.net.2.weight', 'transformer_blocks.18.norm1.weight', 'transformer_blocks.15.attn1.to_q.weight', 'transformer_blocks.15.norm2.weight', 'transformer_blocks.11.attn2.to_v.weight', 'transformer_blocks.17.attn2.to_out.0.weight', 'transformer_blocks.21.ff.net.2.weight', 'transformer_blocks.21.norm2.bias', 'transformer_blocks.16.attn1.to_out.0.weight', 'transformer_blocks.14.attn2.to_out.0.weight', 'transformer_blocks.17.norm2.bias', 'transformer_blocks.12.norm2.bias', 'transformer_blocks.21.attn1.to_out.0.weight', 'transformer_blocks.10.attn2.to_k.weight', 'transformer_blocks.14.norm1.bias', 'transformer_blocks.21.ff.net.0.proj.bias', 'transformer_blocks.11.norm3.bias', 'transformer_blocks.13.norm3.weight', 'transformer_blocks.13.norm1.weight', 'transformer_blocks.12.ff.net.2.weight', 'transformer_blocks.11.attn2.to_q.weight', 'transformer_blocks.17.attn2.to_q.weight', 'transformer_blocks.15.attn2.to_k.weight', 'transformer_blocks.16.attn2.to_out.0.weight', 'transformer_blocks.16.attn1.to_v.weight', 'transformer_blocks.18.attn2.to_q.weight', 'transformer_blocks.22.attn1.to_v.weight', 'transformer_blocks.18.attn2.to_k.weight', 'transformer_blocks.15.attn2.to_q.weight', 'transformer_blocks.19.attn2.to_v.weight', 'transformer_blocks.13.ff.net.0.proj.bias', 'transformer_blocks.21.ff.net.2.bias', 'transformer_blocks.22.ff.net.2.weight', 'transformer_blocks.16.ff.net.0.proj.bias', 'transformer_blocks.10.norm2.weight', 'transformer_blocks.16.attn2.to_k.weight', 'transformer_blocks.11.attn1.to_k.weight', 'transformer_blocks.11.attn1.to_v.weight', 'transformer_blocks.18.ff.net.0.proj.weight', 'transformer_blocks.15.attn1.to_out.0.weight', 'transformer_blocks.12.attn2.to_k.weight', 'transformer_blocks.10.attn1.to_out.0.weight', 'transformer_blocks.17.attn1.to_v.weight', 'transformer_blocks.22.ff.net.2.bias', 'transformer_blocks.18.attn1.to_k.weight', 'transformer_blocks.18.norm2.weight', 'transformer_blocks.11.attn1.to_q.weight', 'transformer_blocks.11.norm3.weight', 'transformer_blocks.16.norm1.weight', 'transformer_blocks.21.norm3.weight', 'transformer_blocks.12.ff.net.0.proj.weight', 'transformer_blocks.20.norm3.bias', 'transformer_blocks.22.attn2.to_q.weight', 'transformer_blocks.14.attn1.to_q.weight', 'transformer_blocks.21.norm2.weight', 'global_proj.0.weight', 'global_proj.2.weight', 'transformer_blocks.18.attn2.to_out.0.weight', 'transformer_blocks.16.norm3.weight', 'transformer_blocks.23.attn2.to_out.0.weight', 'transformer_blocks.21.ff.net.0.proj.weight', 'transformer_blocks.23.norm3.bias', 'transformer_blocks.16.ff.net.2.bias', 'transformer_blocks.18.ff.net.0.proj.bias', 'transformer_blocks.16.ff.net.2.weight', 'transformer_blocks.23.ff.net.2.bias', 'transformer_blocks.10.ff.net.0.proj.bias', 'transformer_blocks.17.norm3.bias', 'transformer_blocks.18.norm1.bias', 'transformer_blocks.13.attn2.to_q.weight', 'transformer_blocks.13.ff.net.2.bias', 'transformer_blocks.18.norm2.bias', 'transformer_blocks.17.norm1.bias', 'transformer_blocks.10.ff.net.2.bias', 'transformer_blocks.16.norm3.bias', 'transformer_blocks.20.attn1.to_v.weight', 'transformer_blocks.19.attn1.to_out.0.weight', 'transformer_blocks.11.norm2.weight', 'transformer_blocks.21.norm3.bias', 'cross_attention_proj.0.weight', 'transformer_blocks.13.ff.net.0.proj.weight', 'transformer_blocks.19.norm2.weight', 'transformer_blocks.12.attn2.to_v.weight', 'transformer_blocks.21.attn1.to_k.weight', 'transformer_blocks.22.norm3.weight', 'transformer_blocks.12.ff.net.0.proj.bias', 'transformer_blocks.20.ff.net.2.bias', 'transformer_blocks.14.ff.net.0.proj.weight', 'transformer_blocks.22.attn1.to_k.weight', 'transformer_blocks.18.attn1.to_out.0.weight', 'transformer_blocks.10.attn1.to_k.weight', 'transformer_blocks.10.attn2.to_q.weight', 'transformer_blocks.20.attn1.to_k.weight', 'transformer_blocks.10.ff.net.0.proj.weight', 'transformer_blocks.19.norm3.bias', 'transformer_blocks.19.norm1.bias', 'transformer_blocks.23.attn1.to_v.weight', 'transformer_blocks.13.attn1.to_out.0.weight', 'transformer_blocks.17.ff.net.2.bias', 'transformer_blocks.11.ff.net.0.proj.bias', 'transformer_blocks.17.attn2.to_k.weight', 'transformer_blocks.15.attn1.to_k.weight', 'transformer_blocks.23.norm1.bias', 'transformer_blocks.23.norm3.weight', 'transformer_blocks.10.norm3.bias', 'transformer_blocks.19.norm3.weight']\n",
      "- This IS expected if you are initializing StableAudioDiTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing StableAudioDiTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of StableAudioDiTModel were not initialized from the model checkpoint at ./assets/stable_audio/ckpt/transformer_lip_synch_text and are newly initialized because the shapes did not match:\n",
      "- postprocess_conv.weight: found shape torch.Size([64, 64, 1]) in the checkpoint and torch.Size([1952, 1952, 1]) in the model instantiated\n",
      "- preprocess_conv.weight: found shape torch.Size([64, 64, 1]) in the checkpoint and torch.Size([1952, 1952, 1]) in the model instantiated\n",
      "- proj_in.weight: found shape torch.Size([1536, 64]) in the checkpoint and torch.Size([1536, 1952]) in the model instantiated\n",
      "- proj_out.weight: found shape torch.Size([64, 1536]) in the checkpoint and torch.Size([1952, 1536]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Restored vaflow ckpt from ./log/2025_12_28-22_22_24-vaflow_noise_lip_synch_text_v2c_synthesized/ckpt/epoch=0199-step=6.00e+03.ckpt\n",
      "VAFlow model loaded!\n",
      "self.encoder_type ECA\n",
      "RawNet model loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading MotionFormer config from /home/chengxin/chengxin/vssflow/feature/synchformer/divided_224_16x4.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP model loaded!\n",
      "Synchformer model loaded!\n",
      "All models loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# ========== 加载模型 ==========\n",
    "print(\"Loading models...\")\n",
    "\n",
    "# 加载VAFlow模型\n",
    "vaflow_model = VAFlow(\n",
    "    ckpt_dir_image_encoder=CONFIG[\"ckpt_dir_image_encoder\"],\n",
    "    ckpt_dir_audio_dit=CONFIG[\"ckpt_dir_audio_dit\"],\n",
    "    ckpt_dir_vocoder=CONFIG[\"ckpt_dir_vocoder\"],\n",
    "    vaflow_ckpt_path=CONFIG[\"vaflow_ckpt_path\"],\n",
    "    phone_ebd_dim=CONFIG[\"phone_ebd_dim\"],\n",
    "    cond_feat_dim=CONFIG[\"cond_feat_dim\"],\n",
    "    lip_feat_dim=CONFIG[\"lip_feat_dim\"],\n",
    "    synch_feat_dim=CONFIG[\"synch_feat_dim\"],\n",
    "    dit_num_layers=CONFIG[\"dit_num_layers\"],\n",
    "    ckpt_dir_vae=CONFIG['ckpt_dir_vae'],\n",
    "    vae_latent_scaling_factor=CONFIG[\"vae_latent_scaling_factor\"],\n",
    "    original_channel=CONFIG[\"original_channel\"],\n",
    "    audio_duration_sec=CONFIG[\"audio_duration_sec\"],\n",
    "    audio_length_per_sec=CONFIG[\"audio_length_per_sec\"],\n",
    "    scale_factor=CONFIG[\"scale_factor\"],\n",
    "    guidance_scale=CONFIG[\"guidance_scale\"],\n",
    "    sample_steps=CONFIG[\"sample_steps\"],\n",
    "    sample_method=CONFIG[\"sample_method\"],\n",
    "    audio_sample_rate=CONFIG[\"audio_sample_rate\"],\n",
    "    num_samples_per_prompt=CONFIG[\"num_samples_per_prompt\"],\n",
    "    resume_training=False,\n",
    "    ignore_keys=[],\n",
    ")\n",
    "\n",
    "vaflow_model = vaflow_model.to(CONFIG[\"device\"])\n",
    "vaflow_model.eval()\n",
    "print(\"VAFlow model loaded!\")\n",
    "\n",
    "# 加载RawNet模型（用于提取speaker embedding）\n",
    "rawnet_model = RawNet3(\n",
    "    Bottle2neck,\n",
    "    model_scale=8,\n",
    "    context=True,\n",
    "    summed=True,\n",
    "    encoder_type=\"ECA\",\n",
    "    nOut=256,\n",
    "    out_bn=False,\n",
    "    sinc_stride=10,\n",
    "    log_sinc=True,\n",
    "    norm_sinc=\"mean\",\n",
    "    grad_mult=1,\n",
    ")\n",
    "rawnet_model.load_state_dict(torch.load(CONFIG[\"rawnet_ckpt_path\"], map_location=\"cpu\")[\"model\"])\n",
    "rawnet_model = rawnet_model.to(CONFIG[\"device\"])\n",
    "rawnet_model.eval()\n",
    "print(\"RawNet model loaded!\")\n",
    "\n",
    "clip_model = CLIPViT(CONFIG[\"ckpt_dir_image_encoder\"])\n",
    "clip_model = clip_model.to(CONFIG[\"device\"])\n",
    "clip_model.eval()\n",
    "print(\"CLIP model loaded!\")\n",
    "\n",
    "synchformer_model = Synchformer().to(CONFIG[\"device\"]).eval()\n",
    "sd = torch.load(CONFIG[\"synchformer_ckpt_path\"], weights_only=True, map_location=CONFIG[\"device\"])\n",
    "synchformer_model.load_state_dict(sd)\n",
    "\n",
    "print(\"Synchformer model loaded!\")\n",
    "print(\"All models loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dc109a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction functions defined!\n"
     ]
    }
   ],
   "source": [
    "# ========== 特征提取函数 ==========\n",
    "\n",
    "def text_to_phoneme_ids(text, token_list_path, \n",
    "                        speech_start_sec = 0, \n",
    "                        speech_end_sec = 10,\n",
    "                        audio_duration_sec=10, \n",
    "                        audio_length_per_sec=25):\n",
    "    \"\"\"将文本转换为phoneme id序列\"\"\"\n",
    "    # 加载phoneme到id的映射\n",
    "    with open(token_list_path, 'r') as f:\n",
    "        phoneme2id = yaml.safe_load(f)\n",
    "    \n",
    "    # 文本清理和phoneme转换\n",
    "    def add_spaces_around_digits(input_string):\n",
    "        return re.sub(r'(?<=\\d)(?=[a-zA-Z])|(?<=[a-zA-Z])(?=\\d)', r' \\g<0> ', input_string)\n",
    "    \n",
    "    cleaner = TextCleaner(\"tacotron\")\n",
    "    tokenizer = PhonemeTokenizer(g2p_type=\"g2p_en_no_space\", non_linguistic_symbols=None)\n",
    "    \n",
    "    # 处理文本\n",
    "    text = add_spaces_around_digits(text)\n",
    "    text = cleaner(text)\n",
    "    phonemes = [\"<blank>\"] + tokenizer.text2tokens(text) + [\"<blank>\"]\n",
    "    \n",
    "    # 转换为id\n",
    "    phone_ids = [phoneme2id.get(p, phoneme2id.get('<blank>', 0)) for p in phonemes]\n",
    "    phone_ids = torch.tensor(phone_ids, dtype=torch.long)\n",
    "\n",
    "    # Speech的长度\n",
    "    target_len = int((speech_end_sec - speech_start_sec) * audio_length_per_sec)\n",
    "    repeat_factor = math.ceil(target_len / phone_ids.shape[0])\n",
    "    phone_ids = phone_ids.repeat_interleave(repeat_factor, dim=0)   \n",
    "    phone_ids = nn_func.interpolate(phone_ids[None, None, :].float(), size=target_len, mode=\"nearest\").squeeze().long()\n",
    "    phone_ids = torch.concat([torch.zeros([int(speech_start_sec * audio_length_per_sec)]), phone_ids])\n",
    "    \n",
    "    \n",
    "    # 调整长度到latent_length\n",
    "    latent_length = int(audio_duration_sec * audio_length_per_sec)\n",
    "    if len(phone_ids) > latent_length:\n",
    "        phone_ids = phone_ids[:latent_length]\n",
    "    else:\n",
    "        phone_ids = torch.nn.functional.pad(phone_ids, (0, latent_length - len(phone_ids)), mode='constant', value=0)\n",
    "    \n",
    "    return phone_ids, phonemes\n",
    "\n",
    "\n",
    "def extract_ref_audio_embedding(audio_path, rawnet_model, device, n_samples=48000, n_segments=10):\n",
    "    \"\"\"从reference audio提取speaker embedding\"\"\"\n",
    "    original_audio_path = audio_path\n",
    "    temp_file_created = False\n",
    "    \n",
    "    # 读取音频\n",
    "    audio, sample_rate = sf.read(audio_path)\n",
    "    if len(audio.shape) > 1:\n",
    "        audio = audio.mean(axis=1)  # 转为单声道\n",
    "    \n",
    "    # 重采样到16kHz（如果需要）\n",
    "    if sample_rate != 16000:\n",
    "        import librosa\n",
    "        audio = librosa.resample(audio, orig_sr=sample_rate, target_sr=16000)\n",
    "        # 保存临时16kHz文件\n",
    "        import tempfile\n",
    "        tmp_path = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)\n",
    "        sf.write(tmp_path.name, audio, 16000)\n",
    "        audio_path = tmp_path.name\n",
    "        tmp_path.close()\n",
    "        temp_file_created = True\n",
    "    \n",
    "    # 提取embedding（extract_speaker_embd需要文件路径）\n",
    "    embedding = extract_speaker_embd(\n",
    "        rawnet_model,\n",
    "        audio_path,\n",
    "        n_samples=n_samples,\n",
    "        n_segments=n_segments,\n",
    "        gpu=(device != \"cpu\")\n",
    "    )\n",
    "    \n",
    "    # 清理临时文件\n",
    "    if temp_file_created:\n",
    "        try:\n",
    "            os.unlink(audio_path)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # 返回平均embedding\n",
    "    if isinstance(embedding, torch.Tensor):\n",
    "        return embedding.mean(0).cpu().numpy()\n",
    "    else:\n",
    "        return embedding.mean(0)\n",
    "\n",
    "\n",
    "def extract_video_clip_features(video_path, clip_model, device, video_process_config):\n",
    "    \"\"\"从video提取CLIP特征\"\"\"\n",
    "    # 获取视频帧\n",
    "    frames = get_video_frames(video_path, video_process_config, backend=\"decord\")\n",
    "    frames = frames.unsqueeze(0).to(device)  # [1, F, C, H, W]\n",
    "    \n",
    "    # 提取特征\n",
    "    with torch.no_grad():\n",
    "        features = encode_image(clip_model, frames, use_projection=False)  # [1, F, C]\n",
    "    \n",
    "    return features.squeeze(0).cpu()  # [F, C]\n",
    "\n",
    "\n",
    "def extract_video_synchformer_features(video_path, synchformer_model, device, audio_length=10.0):\n",
    "    \"\"\"从video提取Synchformer特征\"\"\"\n",
    "    # 创建临时数据集\n",
    "    dataset = VideoDataset([Path(video_path)], duration_sec=audio_length)\n",
    "    data = dataset.sample(0)\n",
    "    \n",
    "    sync_video = data['sync_video'].unsqueeze(0).to(device)  # [1, T, C, H, W]\n",
    "    \n",
    "    # 提取特征\n",
    "    with torch.no_grad():\n",
    "        sync_features = encode_video_with_sync(synchformer_model, sync_video)  # [1, S, T, D]\n",
    "    \n",
    "    return sync_features.squeeze(0).cpu()  # [S, T, D]\n",
    "\n",
    "\n",
    "print(\"Feature extraction functions defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04dacca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference function defined!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from moviepy import VideoFileClip, AudioFileClip\n",
    "import numpy as np\n",
    "\n",
    "def infer_audio_speech(\n",
    "    video_path,\n",
    "    transcript,\n",
    "    speech_start_sec,\n",
    "    speech_end_sec,\n",
    "    ref_audio_path,\n",
    "    output_dir=\"./inference_output\",\n",
    "    gen_num=1,\n",
    "    seed=0,\n",
    "    device=None\n",
    "):\n",
    "    \"\"\"\n",
    "    推理函数：生成audio和speech\n",
    "    \n",
    "    Args:\n",
    "        video_path: 视频文件路径\n",
    "        transcript: 文本转录\n",
    "        ref_audio_path: 参考音频路径（用于speaker embedding）\n",
    "        output_dir: 输出目录\n",
    "        device: 设备（默认使用CONFIG中的device）\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = CONFIG[\"device\"]\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Processing video: {video_path}\")\n",
    "    print(f\"Transcript: {transcript}\")\n",
    "    print(f\"Reference audio: {ref_audio_path}\")\n",
    "    \n",
    "    # ========== 1. 提取所有特征 ==========\n",
    "    print(\"\\n[1/5] Extracting phoneme features...\")\n",
    "    if transcript is not None:\n",
    "        phone_id, phone_seq = text_to_phoneme_ids(\n",
    "            transcript,\n",
    "            CONFIG[\"token_list_path\"],\n",
    "            speech_start_sec = speech_start_sec, \n",
    "            speech_end_sec = speech_end_sec,\n",
    "            audio_duration_sec = CONFIG[\"audio_duration_sec\"], \n",
    "            audio_length_per_sec = CONFIG[\"audio_length_per_sec\"]\n",
    "        )\n",
    "        phone_id = phone_id.unsqueeze(0).to(torch.int).to(device)  # [1, latent_length]\n",
    "    else:\n",
    "        phone_id = torch.zeros([1,CONFIG[\"audio_duration_sec\"] * CONFIG[\"audio_length_per_sec\"] ]).to(torch.int).to(device)\n",
    "    print(f\"Phone ID shape: {phone_id.shape}\")\n",
    "\n",
    "        \n",
    "    print(\"\\n[2/5] Extracting reference audio embedding...\")\n",
    "    if ref_audio_path is not None:\n",
    "        ref_audio_ebd = extract_ref_audio_embedding(\n",
    "            ref_audio_path,\n",
    "            rawnet_model,\n",
    "            device\n",
    "        )\n",
    "        ref_audio_ebd = torch.tensor(ref_audio_ebd, dtype=torch.float32).unsqueeze(0).to(device)  # [1, 256]\n",
    "    else:\n",
    "        ref_audio_ebd = torch.zeros([1, 256]).to(device)\n",
    "    print(f\"Reference audio embedding shape: {ref_audio_ebd.shape}\")\n",
    "\n",
    "    \n",
    "    print(\"\\n[3/5] Extracting CLIP video features...\")\n",
    "    video_feat = extract_video_clip_features(\n",
    "        video_path,\n",
    "        clip_model,\n",
    "        device,\n",
    "        VIDEO_PROCESS_CONFIG\n",
    "    )\n",
    "    video_feat = video_feat.unsqueeze(0).to(device)  # [1, F, C]\n",
    "    print(f\"Video CLIP features shape: {video_feat.shape}\")\n",
    "    \n",
    "    print(\"\\n[4/5] Extracting Synchformer features...\")\n",
    "    synch_feature = extract_video_synchformer_features(\n",
    "        video_path,\n",
    "        synchformer_model,\n",
    "        device,\n",
    "        CONFIG[\"audio_duration_sec\"]\n",
    "    )\n",
    "    synch_feature = synch_feature.unsqueeze(0).to(device)  # [1, S, T, D]\n",
    "    print(f\"Synchformer features shape: {synch_feature.shape}\")\n",
    "    \n",
    "    # ========== 2. 准备模型输入 ==========\n",
    "    print(\"\\n[5/5] Preparing model inputs...\")\n",
    "    batch_size = 1\n",
    "    latent_length = int(CONFIG[\"audio_duration_sec\"] * CONFIG[\"audio_length_per_sec\"])\n",
    "    \n",
    "    # 处理video features\n",
    "    video_feat = torch.nn.functional.interpolate(\n",
    "        video_feat.permute(0, 2, 1),\n",
    "        size=latent_length,\n",
    "        mode='nearest'\n",
    "    )  # [1, 768, latent_length]\n",
    "    video_feat = video_feat.transpose(1, 2)  # [1, latent_length, 768]\n",
    "    \n",
    "    # 处理synchformer features\n",
    "    synch_feature = synch_feature.reshape([batch_size, -1, synch_feature.shape[-1]])  # [1, S*T, D]\n",
    "    synch_feature = synch_feature.transpose(1, 2)  # [1, D, S*T]\n",
    "    synch_feature = torch.nn.functional.interpolate(\n",
    "        synch_feature,\n",
    "        size=latent_length,\n",
    "        mode='nearest-exact'\n",
    "    )  # [1, D, latent_length]\n",
    "    synch_feature = synch_feature.transpose(1, 2)  # [1, latent_length, D]\n",
    "    \n",
    "    # 处理lip features (如果没有，使用零填充)\n",
    "    lip_feature = torch.zeros([batch_size, latent_length, 1024], device=device)\n",
    "    \n",
    "    # 投影特征\n",
    "    video_feat_cond = vaflow_model.cond_proj(video_feat)  # [1, latent_length, 768]\n",
    "    ref_speech_cond = vaflow_model.ref_proj(ref_audio_ebd.unsqueeze(1))  # [1, 1, 768]\n",
    "    phone_latent = vaflow_model.phone_embedding(phone_id)  # [1, latent_length, phone_ebd_dim]\n",
    "    synch_feature = vaflow_model.synch_proj(synch_feature)  # [1, latent_length, synch_feat_dim]\n",
    "    lip_feature = vaflow_model.lip_proj(lip_feature)  # [1, latent_length, lip_feat_dim]\n",
    "    \n",
    "    # 拼接条件特征\n",
    "    video_feat_cond     = torch.concat([ref_speech_cond, video_feat_cond], dim = 1)   # [1, 1+latent_length, 768]\n",
    "    cross_attn_cond     = video_feat_cond                                             # [1, 1+latent_length, 768]\n",
    "    cross_attn_cond_uncond = torch.zeros_like(cross_attn_cond)                        # [1, 1+latent_length, 768]\n",
    "    phone_latent = phone_latent.transpose(1, 2)  # [1, phone_ebd_dim, latent_length]\n",
    "    synch_feature = synch_feature.transpose(1, 2)  # [1, synch_feat_dim, latent_length]\n",
    "    lip_feature = lip_feature.transpose(1, 2)  # [1, lip_feat_dim, latent_length]\n",
    "\n",
    "\n",
    "    # print(video_feat_cond.shape, phone_latent.shape, lip_feature.shape, synch_feature.shape)\n",
    "    latent_cond = torch.concat(\n",
    "        [lip_feature, phone_latent, synch_feature],\n",
    "        dim=1\n",
    "    )  # [1, 768 + phone_ebd_dim + lip_feat_dim + synch_feat_dim, latent_length]\n",
    "    latent_uncond = torch.zeros_like(latent_cond)\n",
    "    \n",
    "    # ========== 3. 模型推理 ==========\n",
    "    print(\"\\nRunning inference...\")\n",
    "    wrapped_vaflow = WrappedModel(vaflow_model.vaflow)\n",
    "    solver = ODESolver(velocity_model=wrapped_vaflow)\n",
    "    \n",
    "    for i in range(gen_num):\n",
    "        generator = torch.Generator(device=device).manual_seed(seed + i)\n",
    "        video_latent = torch.randn(\n",
    "            (batch_size, CONFIG[\"original_channel\"], latent_length),\n",
    "            device=device,\n",
    "            generator=generator\n",
    "        )  # [1, original_channel, latent_length]\n",
    "        video_latent = torch.cat([video_latent, latent_cond], dim=1).detach()  # [1, original_channel + cond_dim, latent_length]\n",
    "        \n",
    "        # Rotary embedding\n",
    "        rotary_embedding = get_1d_rotary_pos_embed(\n",
    "            vaflow_model.rotary_embed_dim,\n",
    "            video_latent.shape[2] + 1,\n",
    "            use_real=True,\n",
    "            repeat_interleave_real=False,\n",
    "        )\n",
    "        \n",
    "        # Flow matching采样\n",
    "        time_grid = torch.linspace(0, 1, CONFIG[\"sample_steps\"] + 1).to(device)\n",
    "        with torch.no_grad():\n",
    "            synthetic_samples = solver.sample(\n",
    "                time_grid=time_grid,\n",
    "                x_init=video_latent,\n",
    "                method=CONFIG[\"sample_method\"],\n",
    "                return_intermediates=False,\n",
    "                atol=1e-5,\n",
    "                rtol=1e-5,\n",
    "                step_size=None,\n",
    "                global_cond=ref_speech_cond,\n",
    "                latent_uncond=latent_uncond,\n",
    "                w=CONFIG[\"guidance_scale\"],\n",
    "                c=cross_attn_cond,\n",
    "                cross_attn_uncond=cross_attn_cond_uncond,\n",
    "                rotary_embedding=rotary_embedding,\n",
    "            )\n",
    "        \n",
    "        # ========== 4. 解码生成音频 ==========\n",
    "        print(\"\\nDecoding audio...\")\n",
    "        audio_latent = synthetic_samples  # [1, original_channel + cond_dim, latent_length]\n",
    "        audio_latent = audio_latent[:, :CONFIG[\"original_channel\"], :]  # [1, original_channel, latent_length]\n",
    "        audio_latent = audio_latent / CONFIG[\"scale_factor\"]\n",
    "        \n",
    "        # 重塑为VAE输入格式\n",
    "        audio_latent = audio_latent.reshape([batch_size, -1, 8, audio_latent.shape[-1]])  # [1, 16, 8, latent_length]\n",
    "        audio_latent = audio_latent.transpose(-2, -3).transpose(-2, -1)  # [1, 8, latent_length, 16]\n",
    "        \n",
    "        # VAE解码\n",
    "        with torch.no_grad():\n",
    "            mel_spectrogram = vaflow_model.vae.decode(audio_latent).sample  # [1, 1, mel_length, 64]\n",
    "            gen_audio = vaflow_model.vocoder(mel_spectrogram.squeeze(1))  # [1, audio_length]\n",
    "        \n",
    "        gen_audio = gen_audio.cpu()\n",
    "        \n",
    "        # ========== 5. 保存结果 ==========\n",
    "        video_clip = VideoFileClip(video_path)\n",
    "        video_id = Path(video_path).stem\n",
    "        audio_path = os.path.join(output_dir, f\"{video_id}_generated_{seed+i}.wav\")\n",
    "        output_video_path = os.path.join(output_dir, f\"{video_id}_generated_{seed+i}.mp4\")\n",
    "\n",
    "        torchaudio.save(audio_path, gen_audio, CONFIG[\"audio_sample_rate\"])\n",
    "        new_audio_clip = AudioFileClip(audio_path)\n",
    "        video_clip.audio = new_audio_clip\n",
    "        video_clip.write_videofile(output_video_path, codec=\"libx264\", audio_codec=\"aac\")\n",
    "        video_clip.close()\n",
    "        new_audio_clip.close()\n",
    "\n",
    "    \n",
    "    # return audio_path, gen_audio\n",
    "\n",
    "print(\"Inference function defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f51e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: /home/chengxin/chengxin/vssflow/infer/man_zombie.mp4\n",
      "Transcript: EAT LEAD, ZOMBIE SCUM!\n",
      "Reference audio: /home/chengxin/chengxin/vssflow/infer/man_zombie.wav\n",
      "\n",
      "[1/5] Extracting phoneme features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame_index:  62%|██████▏   | 103/166 [00:16<00:07,  8.66it/s, now=None]/home/chengxin/chengxin/vssflow/feature/RawNet/python/RawNet3/models/RawNet3.py:77: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phone ID shape: torch.Size([1, 250])\n",
      "\n",
      "[2/5] Extracting reference audio embedding...\n",
      "Reference audio embedding shape: torch.Size([1, 256])\n",
      "\n",
      "[3/5] Extracting CLIP video features...\n",
      "Video CLIP features shape: torch.Size([1, 100, 768])\n",
      "\n",
      "[4/5] Extracting Synchformer features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Pad 112 frames /home/chengxin/chengxin/vssflow/infer/man_zombie.mp4 to 250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synchformer features shape: torch.Size([1, 30, 8, 768])\n",
      "\n",
      "[5/5] Preparing model inputs...\n",
      "\n",
      "Running inference...\n",
      "\n",
      "Decoding audio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame_index:  62%|██████▏   | 103/166 [00:51<00:07,  8.66it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building video ./infer/man_zombie/man_zombie_generated_47.mp4.\n",
      "MoviePy - Writing audio in man_zombie_generated_47TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame_index:  62%|██████▏   | 103/166 [00:52<00:07,  8.66it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing video ./infer/man_zombie/man_zombie_generated_47.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame_index:  62%|██████▏   | 103/166 [01:00<00:07,  8.66it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready ./infer/man_zombie/man_zombie_generated_47.mp4\n",
      "\n",
      "Decoding audio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame_index:  62%|██████▏   | 103/166 [01:30<00:07,  8.66it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building video ./infer/man_zombie/man_zombie_generated_48.mp4.\n",
      "MoviePy - Writing audio in man_zombie_generated_48TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame_index:  62%|██████▏   | 103/166 [01:30<00:07,  8.66it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing video ./infer/man_zombie/man_zombie_generated_48.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame_index:  62%|██████▏   | 103/166 [01:44<00:07,  8.66it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready ./infer/man_zombie/man_zombie_generated_48.mp4\n",
      "\n",
      "Decoding audio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame_index:  62%|██████▏   | 103/166 [02:10<00:07,  8.66it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building video ./infer/man_zombie/man_zombie_generated_49.mp4.\n",
      "MoviePy - Writing audio in man_zombie_generated_49TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame_index:  62%|██████▏   | 103/166 [02:10<00:07,  8.66it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing video ./infer/man_zombie/man_zombie_generated_49.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame_index:  62%|██████▏   | 103/166 [02:19<00:07,  8.66it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready ./infer/man_zombie/man_zombie_generated_49.mp4\n",
      "\n",
      "Decoding audio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame_index:  62%|██████▏   | 103/166 [02:51<00:07,  8.66it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building video ./infer/man_zombie/man_zombie_generated_50.mp4.\n",
      "MoviePy - Writing audio in man_zombie_generated_50TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame_index:  62%|██████▏   | 103/166 [02:51<00:07,  8.66it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing video ./infer/man_zombie/man_zombie_generated_50.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame_index:  62%|██████▏   | 103/166 [03:26<00:07,  8.66it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready ./infer/man_zombie/man_zombie_generated_50.mp4\n",
      "\n",
      "Decoding audio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame_index:  62%|██████▏   | 103/166 [03:48<00:07,  8.66it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building video ./infer/man_zombie/man_zombie_generated_51.mp4.\n",
      "MoviePy - Writing audio in man_zombie_generated_51TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame_index:  62%|██████▏   | 103/166 [03:48<00:07,  8.66it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing video ./infer/man_zombie/man_zombie_generated_51.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame_index:  62%|██████▏   | 103/166 [03:55<00:07,  8.66it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready ./infer/man_zombie/man_zombie_generated_51.mp4\n",
      "\n",
      "Decoding audio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame_index:  62%|██████▏   | 103/166 [04:26<00:07,  8.66it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building video ./infer/man_zombie/man_zombie_generated_52.mp4.\n",
      "MoviePy - Writing audio in man_zombie_generated_52TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame_index:  62%|██████▏   | 103/166 [04:26<00:07,  8.66it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing video ./infer/man_zombie/man_zombie_generated_52.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame_index:  62%|██████▏   | 103/166 [04:30<00:07,  8.66it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready ./infer/man_zombie/man_zombie_generated_52.mp4\n",
      "\n",
      "Decoding audio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame_index:  62%|██████▏   | 103/166 [04:58<00:07,  8.66it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building video ./infer/man_zombie/man_zombie_generated_53.mp4.\n",
      "MoviePy - Writing audio in man_zombie_generated_53TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame_index:  62%|██████▏   | 103/166 [04:58<00:07,  8.66it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing video ./infer/man_zombie/man_zombie_generated_53.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame_index:  62%|██████▏   | 103/166 [05:03<00:07,  8.66it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready ./infer/man_zombie/man_zombie_generated_53.mp4\n",
      "\n",
      "Decoding audio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame_index:  62%|██████▏   | 103/166 [05:27<00:07,  8.66it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building video ./infer/man_zombie/man_zombie_generated_54.mp4.\n",
      "MoviePy - Writing audio in man_zombie_generated_54TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame_index:  62%|██████▏   | 103/166 [05:28<00:07,  8.66it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing video ./infer/man_zombie/man_zombie_generated_54.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame_index:  62%|██████▏   | 103/166 [05:35<00:07,  8.66it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready ./infer/man_zombie/man_zombie_generated_54.mp4\n",
      "\n",
      "Decoding audio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame_index:  62%|██████▏   | 103/166 [05:57<00:07,  8.66it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building video ./infer/man_zombie/man_zombie_generated_55.mp4.\n",
      "MoviePy - Writing audio in man_zombie_generated_55TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame_index:  62%|██████▏   | 103/166 [05:57<00:07,  8.66it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing video ./infer/man_zombie/man_zombie_generated_55.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame_index:  62%|██████▏   | 103/166 [06:03<00:07,  8.66it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready ./infer/man_zombie/man_zombie_generated_55.mp4\n",
      "\n",
      "Decoding audio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame_index:  62%|██████▏   | 103/166 [06:28<00:07,  8.66it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building video ./infer/man_zombie/man_zombie_generated_56.mp4.\n",
      "MoviePy - Writing audio in man_zombie_generated_56TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame_index:  62%|██████▏   | 103/166 [06:29<00:07,  8.66it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing video ./infer/man_zombie/man_zombie_generated_56.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame_index:  62%|██████▏   | 103/166 [06:35<00:07,  8.66it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready ./infer/man_zombie/man_zombie_generated_56.mp4\n"
     ]
    }
   ],
   "source": [
    "gen_num = 10\n",
    "\n",
    "#####################./log/2026_01_03-20_43_43-vaflow_noise_lip_synch_text_synthesized2/ckpt/epoch=0399-step=1.20e+04.ckpt\n",
    "# # # transcript = \"EAT LEAD, ZOMBIE SCUM!\"\n",
    "# # # ref_audio_path = \"/home/chengxin/chengxin/vssflow/infer/man_zombie.wav\"\n",
    "# # # # ref_audio_path = None\n",
    "# # # video_path = \"/home/chengxin/chengxin/vssflow/infer/man_zombie.mp4\"\n",
    "# # # speech_start_sec = 0.5\n",
    "# # # speech_end_sec = 2.2\n",
    "# # # output_dir = \"./infer/man_zombie\"\n",
    "# # # os.makedirs(output_dir, exist_ok = True)\n",
    "# # # infer_audio_speech(\n",
    "# # #     video_path=video_path,\n",
    "# # #     transcript=transcript,\n",
    "# # #     speech_start_sec=speech_start_sec,\n",
    "# # #     speech_end_sec=speech_end_sec,\n",
    "# # #     ref_audio_path=ref_audio_path,\n",
    "# # #     output_dir=output_dir,\n",
    "# # #     gen_num=gen_num,\n",
    "# # #     seed=47\n",
    "# # # )\n",
    "\n",
    "\n",
    "######################./log/2025_12_28-22_15_02-vaflow_noise_lip_synch_text_synthesized/ckpt/last.ckpt\n",
    "# # # transcript = \"HI, AND WELCOME TO THE CHANNEL\"\n",
    "# # # ref_audio_path = \"/home/chengxin/chengxin/vssflow/infer/keyboard_woman.wav\"\n",
    "# # # ref_audio_path = None\n",
    "# # # video_path = \"/home/chengxin/chengxin/vssflow/infer/keyboard_woman.mp4\"\n",
    "# # # speech_start_sec = 4.5\n",
    "# # # speech_end_sec = 6.5\n",
    "# # # output_dir = \"./infer/keyboard_woman\"\n",
    "# # # os.makedirs(output_dir, exist_ok = True)\n",
    "# # # infer_audio_speech(\n",
    "# # #     video_path=video_path,\n",
    "# # #     transcript=transcript,\n",
    "# # #     speech_start_sec=speech_start_sec,\n",
    "# # #     speech_end_sec=speech_end_sec,\n",
    "# # #     ref_audio_path=ref_audio_path,\n",
    "# # #     output_dir=output_dir,\n",
    "# # #     gen_num=gen_num,\n",
    "# # #     seed=47\n",
    "# # # )\n",
    "\n",
    "#####################./log/2026_01_03-20_43_43-vaflow_noise_lip_synch_text_synthesized2/ckpt/epoch=0399-step=1.20e+04.ckpt\n",
    "# # # transcript = \"WHEN GETTING IN THERE, I WANT NO BULLSHIT\"\n",
    "# # # ref_audio_path = \"/home/chengxin/chengxin/vssflow/infer/car_police.wav\"\n",
    "# # # # ref_audio_path = None\n",
    "# # # video_path = \"/home/chengxin/chengxin/vssflow/infer/car_police.mp4\"\n",
    "# # # speech_start_sec = 2.7\n",
    "# # # speech_end_sec = 5.0\n",
    "# # # output_dir = \"./infer/car_police\"\n",
    "# # # os.makedirs(output_dir, exist_ok = True)\n",
    "# # # infer_audio_speech(\n",
    "# # #     video_path=video_path,\n",
    "# # #     transcript=transcript,\n",
    "# # #     speech_start_sec=speech_start_sec,\n",
    "# # #     speech_end_sec=speech_end_sec,\n",
    "# # #     ref_audio_path=ref_audio_path,\n",
    "# # #     output_dir=output_dir,\n",
    "# # #     gen_num=gen_num,\n",
    "# # #     seed=52\n",
    "# # # )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "888f1600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: /public/huggingface-datasets/Aimind-dataset-share/Audioset/data/balanced_train/balanced_train_18/CZmiS6QsfJU_220.0.mp4\n",
      "Transcript: I'M GOING TO DIE.\n",
      "Reference audio: None\n",
      "\n",
      "[1/5] Extracting phoneme features...\n",
      "Phone ID shape: torch.Size([1, 250])\n",
      "\n",
      "[2/5] Extracting reference audio embedding...\n",
      "Reference audio embedding shape: torch.Size([1, 256])\n",
      "\n",
      "[3/5] Extracting CLIP video features...\n",
      "Video CLIP features shape: torch.Size([1, 100, 768])\n",
      "\n",
      "[4/5] Extracting Synchformer features...\n",
      "Synchformer features shape: torch.Size([1, 30, 8, 768])\n",
      "\n",
      "[5/5] Preparing model inputs...\n",
      "\n",
      "Running inference...\n",
      "\n",
      "Decoding audio...\n",
      "MoviePy - Building video ./infer/yihan/CZmiS6QsfJU_220.0_generated_52.mp4.\n",
      "MoviePy - Writing audio in CZmiS6QsfJU_220.0_generated_52TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing video ./infer/yihan/CZmiS6QsfJU_220.0_generated_52.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready ./infer/yihan/CZmiS6QsfJU_220.0_generated_52.mp4\n"
     ]
    }
   ],
   "source": [
    "gen_num = 1\n",
    "output_dir = \"./infer/yihan\"\n",
    "ref_audio_path = None\n",
    "os.makedirs(output_dir, exist_ok = True)\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('/home/chengxin/chengxin/vssflow/data/av_metadata_500.tsv', 'r', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    for row in reader:\n",
    "        id, video_path, transcript, speech_start_sec, speech_end_sec = row\n",
    "        transcript = transcript.upper()\n",
    "        speech_start_sec = float(speech_start_sec)\n",
    "        speech_end_sec = float(speech_end_sec)\n",
    "        \n",
    "        try:\n",
    "            infer_audio_speech(\n",
    "                video_path=video_path,\n",
    "                transcript=transcript,\n",
    "                speech_start_sec=speech_start_sec,\n",
    "                speech_end_sec=speech_end_sec,\n",
    "                ref_audio_path=ref_audio_path,\n",
    "                output_dir=output_dir,\n",
    "                gen_num=gen_num,\n",
    "                seed=52\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(id, e)\n",
    "            continue\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "860cd341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/public/huggingface-datasets/Aimind-dataset-share/Audioset/data/balanced_train/balanced_train_18/CZmiS6QsfJU_220.0.mp4'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a59fca",
   "metadata": {},
   "source": [
    "# VAFlow 推理脚本使用说明\n",
    "\n",
    "## 功能\n",
    "本脚本实现了完整的VAFlow推理流程：\n",
    "1. **文本转Phoneme**: 将transcript文本转换为phoneme id序列\n",
    "2. **提取Reference Audio特征**: 使用RawNet3提取speaker embedding\n",
    "3. **提取Video特征**: \n",
    "   - 使用CLIP提取视频帧的视觉特征\n",
    "   - 使用Synchformer提取视频的同步特征\n",
    "4. **模型推理**: 使用VAFlow模型生成音频和语音\n",
    "5. **保存结果**: 将生成的音频保存为WAV文件\n",
    "\n",
    "## 使用步骤\n",
    "\n",
    "1. **配置路径**: 在Cell 1中修改模型checkpoint路径\n",
    "2. **加载模型**: 运行Cell 2加载所有必要的模型\n",
    "3. **准备输入**: 在Cell 5中设置：\n",
    "   - `video_path`: 输入视频文件路径\n",
    "   - `transcript`: 文本转录\n",
    "   - `ref_audio_path`: 参考音频路径（用于speaker embedding）\n",
    "4. **运行推理**: 执行Cell 5进行推理\n",
    "\n",
    "## 注意事项\n",
    "\n",
    "- 确保所有模型checkpoint路径正确\n",
    "- Reference audio应该是16kHz单声道，如果不是会自动重采样\n",
    "- Video应该是MP4格式\n",
    "- 生成的音频长度为10秒（可在CONFIG中修改）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1256b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vagen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
