{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cacd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from worker.vaflow_sda_dit_noise_text_mel import VAFlow\n",
    "from worker.dp_tts import DurationPredictor\n",
    "from util.inference import infer_videos\n",
    "\n",
    "\n",
    "device = 'cuda:0'\n",
    "with open('/home/chengxin/chengxin/vasflow/config/model_vaflow.yaml', 'r') as f:\n",
    "    vaflow_config = yaml.safe_load(f)\n",
    "vaflow = VAFlow(**vaflow_config).to(device)\n",
    "\n",
    "\n",
    "with open('/home/chengxin/chengxin/vasflow/config/model_dp.yaml', 'r') as f:\n",
    "    dp_config = yaml.safe_load(f)\n",
    "dp = DurationPredictor(**dp_config).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df674f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 250, 23])\n",
      "torch.Size([1, 23]) torch.Size([24]) torch.Size([1, 250, 23]) torch.Size([250, 24])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# keyboard_women     epoch0007 0019\n",
    "va_paths = ['/home/chengxin/chengxin/vasflow/vis/videos/keyboard_woman.mp4']\n",
    "text_seq = ['Hi, and welcome to the channel']\n",
    "speech_start_durations = [4.5]   \n",
    "video_process_config = {'duration':10.0, 'resize_input_size': [224, 224], 'target_sampling_rate': 10, 'raw_duration_min_threshold':0.05}\n",
    "\n",
    "# 可以没有。speech_durations和avhubert_feature_paths只需要有一个\n",
    "speech_durations = [2]\n",
    "avhubert_feature_paths = None #['/nfs-04/yuyue/visualtts_datasets/lrs2/lrs2_for_espnet/preprocessed_data/lip_feature/6330311066473698535_27418_00000.npy']\n",
    "ref_audio_ebd_paths = None #['/home/chengxin/chengxin/Dataset_Sound/LRS2/speaker_emb/6330311066473698535_27418/6330311066473698535_27418_00000.npy']\n",
    "\n",
    "\n",
    "\n",
    "infer_videos(\n",
    "    vaflow_model = vaflow, \n",
    "    dp_model = dp,\n",
    "    save_path='/home/chengxin/chengxin/vasflow', \n",
    "    mp4_paths=va_paths, \n",
    "    video_process_config = video_process_config, \n",
    "    text_seq = text_seq,                              \n",
    "    avhubert_feature_paths = avhubert_feature_paths,  # 抽取avhubert_feature的代码没有实现\n",
    "    speech_durations=speech_durations,\n",
    "    speech_start_durations=speech_start_durations,\n",
    "    ref_audio_ebd_paths = ref_audio_ebd_paths,        # 抽取ref_audio_ebd的代码没有实现\n",
    "    num_samples_per_prompt = 10,\n",
    "    guidance_scale = 3,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea360bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# car_police      epoch0007 0019\n",
    "va_paths = ['/home/chengxin/chengxin/vasflow/vis/videos/car_police.mp4']\n",
    "text_seq = ['We get in there, I want no bullshit.']\n",
    "speech_start_durations = [2.8]   \n",
    "video_process_config = {'duration':10.0, 'resize_input_size': [224, 224], 'target_sampling_rate': 10, 'raw_duration_min_threshold':0.05}\n",
    "\n",
    "# 可以没有。speech_durations和avhubert_feature_paths只需要有一个\n",
    "speech_durations = [2.5]\n",
    "avhubert_feature_paths = None #['/nfs-04/yuyue/visualtts_datasets/lrs2/lrs2_for_espnet/preprocessed_data/lip_feature/6330311066473698535_27418_00000.npy']\n",
    "ref_audio_ebd_paths = None #['/home/chengxin/chengxin/Dataset_Sound/LRS2/speaker_emb/6330311066473698535_27418/6330311066473698535_27418_00000.npy']\n",
    "\n",
    "\n",
    "\n",
    "infer_videos(\n",
    "    vaflow_model = vaflow, \n",
    "    dp_model = dp,\n",
    "    save_path='/home/chengxin/chengxin/vasflow/test', \n",
    "    mp4_paths=va_paths, \n",
    "    video_process_config = video_process_config, \n",
    "    text_seq = text_seq,                              \n",
    "    avhubert_feature_paths = avhubert_feature_paths,  # 抽取avhubert_feature的代码没有实现\n",
    "    speech_durations=speech_durations,\n",
    "    speech_start_durations=speech_start_durations,\n",
    "    ref_audio_ebd_paths = ref_audio_ebd_paths,        # 抽取ref_audio_ebd的代码没有实现\n",
    "    device=device,\n",
    "    num_samples_per_prompt = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87e701e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 250, 19])\n",
      "torch.Size([1, 19]) torch.Size([20]) torch.Size([1, 250, 19]) torch.Size([250, 20])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# man_zombie    epoch0019\n",
    "va_paths = ['/home/chengxin/chengxin/vasflow/vis/videos/man_zombie2.mp4']\n",
    "text_seq = ['Eat led, zombies scum.']\n",
    "speech_start_durations = [0.5]   \n",
    "video_process_config = {'duration':10.0, 'resize_input_size': [224, 224], 'target_sampling_rate': 10, 'raw_duration_min_threshold':0.05}\n",
    "\n",
    "# 可以没有。speech_durations和avhubert_feature_paths只需要有一个\n",
    "speech_durations = [1.8]\n",
    "avhubert_feature_paths = None #['/nfs-04/yuyue/visualtts_datasets/lrs2/lrs2_for_espnet/preprocessed_data/lip_feature/6330311066473698535_27418_00000.npy']\n",
    "ref_audio_ebd_paths = None #['/home/chengxin/chengxin/Dataset_Sound/LRS2/speaker_emb/6330311066473698535_27418/6330311066473698535_27418_00000.npy']\n",
    "\n",
    "\n",
    "\n",
    "infer_videos(\n",
    "    vaflow_model = vaflow, \n",
    "    dp_model = dp,\n",
    "    save_path='/home/chengxin/chengxin/vasflow/test', \n",
    "    mp4_paths=va_paths, \n",
    "    video_process_config = video_process_config, \n",
    "    text_seq = text_seq,                              \n",
    "    avhubert_feature_paths = avhubert_feature_paths,  # 抽取avhubert_feature的代码没有实现\n",
    "    speech_durations=speech_durations,\n",
    "    speech_start_durations=speech_start_durations,\n",
    "    ref_audio_ebd_paths = ref_audio_ebd_paths,        # 抽取ref_audio_ebd的代码没有实现\n",
    "    device=device,\n",
    "    guidance_scale = 4,\n",
    "    num_samples_per_prompt = 10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233ae438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os, shutil\n",
    "import torchaudio\n",
    "save_path = '/home/chengxin/chengxin/Dataset_Sound/VGGSound/generated_audios/vasflow/avsync'\n",
    "\n",
    "\n",
    "\n",
    "for file in os.listdir('/home/chengxin/chengxin/Dataset_Sound/VGGSound/dataset/test/avsync'):\n",
    "    path = f'/home/chengxin/chengxin/Dataset_Sound/VGGSound/dataset/test/avsync/{file}'\n",
    "    va_paths = [path]\n",
    "    video_process_config = {'duration':10.0, 'resize_input_size': [224, 224], 'target_sampling_rate': 10, 'raw_duration_min_threshold':0.05}\n",
    "\n",
    "    # 可以没有。speech_durations和avhubert_feature_paths只需要有一个\n",
    "    text_seq = None\n",
    "    speech_durations = None\n",
    "    speech_start_durations = None\n",
    "    avhubert_feature_paths = None #['/nfs-04/yuyue/visualtts_datasets/lrs2/lrs2_for_espnet/preprocessed_data/lip_feature/6330311066473698535_27418_00000.npy']\n",
    "    ref_audio_ebd_paths = None #['/home/chengxin/chengxin/Dataset_Sound/LRS2/speaker_emb/6330311066473698535_27418/6330311066473698535_27418_00000.npy']\n",
    "\n",
    "\n",
    "\n",
    "    infer_videos(\n",
    "        vaflow_model = vaflow, \n",
    "        dp_model = dp,\n",
    "        save_path=save_path, \n",
    "        mp4_paths=va_paths, \n",
    "        video_process_config = video_process_config, \n",
    "        text_seq = text_seq,                              \n",
    "        avhubert_feature_paths = avhubert_feature_paths,  # 抽取avhubert_feature的代码没有实现\n",
    "        speech_durations=speech_durations,\n",
    "        speech_start_durations=speech_start_durations,\n",
    "        ref_audio_ebd_paths = ref_audio_ebd_paths,        # 抽取ref_audio_ebd的代码没有实现\n",
    "        num_samples_per_prompt = 1,\n",
    "        guidance_scale = 3,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "\n",
    "import os, shutil\n",
    "\n",
    "for file in os.listdir(save_path):\n",
    "    shutil.move(f\"/home/chengxin/chengxin/Dataset_Sound/VGGSound/generated_audios/vasflow/avsync/{file}\", \n",
    "                f\"/home/chengxin/chengxin/Dataset_Sound/VGGSound/generated_audios/vasflow/avsync/{file.replace('_00.wav', '.wav')}\")\n",
    "    \n",
    "    # wav, sr = torchaudio.load(f\"/home/chengxin/chengxin/Dataset_Sound/VGGSound/generated_audios/vasflow/avsync/{file.replace('_00.wav', '.wav')}\")\n",
    "    # wav = wav[...:, :int(5*sr)]\n",
    "    # torchaudio.save(f\"/home/chengxin/chengxin/Dataset_Sound/VGGSound/generated_audios/vasflow/avsync/{file.replace('_00.wav', '.wav')}\", wav, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6f558d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9294cb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os, torchaudio\n",
    "from glob import glob\n",
    "\n",
    "for file in os.listdir(\"/home/chengxin/chengxin/Dataset_Sound/VGGSound/generated_audios/bicross/avsync_video\"):\n",
    "    target = file.split('_')[:-3]\n",
    "    target = \"_\".join(target)\n",
    "    wav, sr = torchaudio.load(f\"/home/chengxin/chengxin/Dataset_Sound/VGGSound/generated_audios/bicross/avsync_video/{file}\")\n",
    "    torchaudio.save(f\"/home/chengxin/chengxin/Dataset_Sound/VGGSound/generated_audios/bicross/avsync/{target}.wav\", wav, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3fb6ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01cf46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/chengxin/chengxin/vasflow/test/keyboard_woman_00_replace_audio.mp4.\n",
      "MoviePy - Writing audio in keyboard_woman_00_replace_audioTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /home/chengxin/chengxin/vasflow/test/keyboard_woman_00_replace_audio.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/chengxin/chengxin/vasflow/test/keyboard_woman_00_replace_audio.mp4\n",
      "Moviepy - Building video /home/chengxin/chengxin/vasflow/test/keyboard_woman_01_replace_audio.mp4.\n",
      "MoviePy - Writing audio in keyboard_woman_01_replace_audioTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /home/chengxin/chengxin/vasflow/test/keyboard_woman_01_replace_audio.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/chengxin/chengxin/vasflow/test/keyboard_woman_01_replace_audio.mp4\n",
      "Moviepy - Building video /home/chengxin/chengxin/vasflow/test/keyboard_woman_02_replace_audio.mp4.\n",
      "MoviePy - Writing audio in keyboard_woman_02_replace_audioTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /home/chengxin/chengxin/vasflow/test/keyboard_woman_02_replace_audio.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/chengxin/chengxin/vasflow/test/keyboard_woman_02_replace_audio.mp4\n",
      "Moviepy - Building video /home/chengxin/chengxin/vasflow/test/keyboard_woman_03_replace_audio.mp4.\n",
      "MoviePy - Writing audio in keyboard_woman_03_replace_audioTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /home/chengxin/chengxin/vasflow/test/keyboard_woman_03_replace_audio.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/chengxin/chengxin/vasflow/test/keyboard_woman_03_replace_audio.mp4\n",
      "Moviepy - Building video /home/chengxin/chengxin/vasflow/test/keyboard_woman_04_replace_audio.mp4.\n",
      "MoviePy - Writing audio in keyboard_woman_04_replace_audioTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /home/chengxin/chengxin/vasflow/test/keyboard_woman_04_replace_audio.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/chengxin/chengxin/vasflow/test/keyboard_woman_04_replace_audio.mp4\n",
      "Moviepy - Building video /home/chengxin/chengxin/vasflow/test/keyboard_woman_05_replace_audio.mp4.\n",
      "MoviePy - Writing audio in keyboard_woman_05_replace_audioTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /home/chengxin/chengxin/vasflow/test/keyboard_woman_05_replace_audio.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/chengxin/chengxin/vasflow/test/keyboard_woman_05_replace_audio.mp4\n",
      "Moviepy - Building video /home/chengxin/chengxin/vasflow/test/keyboard_woman_06_replace_audio.mp4.\n",
      "MoviePy - Writing audio in keyboard_woman_06_replace_audioTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /home/chengxin/chengxin/vasflow/test/keyboard_woman_06_replace_audio.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/chengxin/chengxin/vasflow/test/keyboard_woman_06_replace_audio.mp4\n",
      "Moviepy - Building video /home/chengxin/chengxin/vasflow/test/keyboard_woman_07_replace_audio.mp4.\n",
      "MoviePy - Writing audio in keyboard_woman_07_replace_audioTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /home/chengxin/chengxin/vasflow/test/keyboard_woman_07_replace_audio.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/chengxin/chengxin/vasflow/test/keyboard_woman_07_replace_audio.mp4\n",
      "Moviepy - Building video /home/chengxin/chengxin/vasflow/test/keyboard_woman_08_replace_audio.mp4.\n",
      "MoviePy - Writing audio in keyboard_woman_08_replace_audioTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /home/chengxin/chengxin/vasflow/test/keyboard_woman_08_replace_audio.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/chengxin/chengxin/vasflow/test/keyboard_woman_08_replace_audio.mp4\n",
      "Moviepy - Building video /home/chengxin/chengxin/vasflow/test/keyboard_woman_09_replace_audio.mp4.\n",
      "MoviePy - Writing audio in keyboard_woman_09_replace_audioTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /home/chengxin/chengxin/vasflow/test/keyboard_woman_09_replace_audio.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/chengxin/chengxin/vasflow/test/keyboard_woman_09_replace_audio.mp4\n"
     ]
    }
   ],
   "source": [
    "import moviepy.editor as mpy\n",
    "\n",
    "# 输入视频和音频路径\n",
    "name = 'keyboard_woman'\n",
    "for idx in range(10):\n",
    "    input_video = f'/home/chengxin/chengxin/vasflow/vis/videos/{name}.mp4'\n",
    "    input_audio = f'/home/chengxin/chengxin/vasflow/vis/videos/{name}_{idx:02d}.wav'  # 假设wav和npy同名同目录\n",
    "    output_video = input_audio.replace('.wav', '_replace_audio.mp4')\n",
    "\n",
    "    # 加载视频和音频\n",
    "    video_clip = mpy.VideoFileClip(input_video)\n",
    "    audio_clip = mpy.AudioFileClip(input_audio)\n",
    "    audio_clip = audio_clip.subclip(0, video_clip.duration)\n",
    "\n",
    "\n",
    "    # 替换音频\n",
    "    video_with_new_audio = video_clip.set_audio(audio_clip)\n",
    "    video_with_new_audio.write_videofile(output_video, codec='libx264', audio_codec='aac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da299536",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# va_paths = ['/home/chengxin/chengxin/vasflow/test/us_wm_veo_3_a-medium-shot-frames-an-old-sailor-his-knitted-blue-sailor-hat_LLpQrIw.mp4']\n",
    "# text_seq = ['This ocean, is a force, a wild, untamed might. .  And she commands you all, with every breaking light.']\n",
    "# speech_start_durations = [0]   \n",
    "# video_process_config = {'duration':10.0, 'resize_input_size': [224, 224], 'target_sampling_rate': 10, 'raw_duration_min_threshold':0.05}\n",
    "\n",
    "# # 可以没有。speech_durations和avhubert_feature_paths只需要有一个\n",
    "# speech_durations = [8]\n",
    "# avhubert_feature_pahs = None #['/nfs-04/yuyue/visualtts_datasets/lrs2/lrs2_for_espnet/preprocessed_data/lip_feature/6330311066473698535_27418_00000.npy']\n",
    "# ref_audio_ebd_paths = ['/home/chengxin/chengxin/Dataset_Sound/LRS2/speaker_emb/6330311066473698535_27418/6330311066473698535_27418_00000.npy']\n",
    "\n",
    "\n",
    "\n",
    "# infer_videos(\n",
    "#     vaflow_model = vaflow, \n",
    "#     dp_model = dp,\n",
    "#     save_path='/home/chengxin/chengxin/vasflow/test', \n",
    "#     mp4_paths=va_paths, \n",
    "#     video_process_config = video_process_config, \n",
    "#     text_seq = text_seq,                              \n",
    "#     avhubert_feature_paths = avhubert_feature_paths,  # 抽取avhubert_feature的代码没有实现\n",
    "#     speech_durations=speech_durations,\n",
    "#     speech_start_durations=speech_start_durations,\n",
    "#     ref_audio_ebd_paths = ref_audio_ebd_paths,        # 抽取ref_audio_ebd的代码没有实现\n",
    "#     device=device,\n",
    "#     num_samples_per_prompt = 10,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6957af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from util.get_dict import PhonemeTokenizer\n",
    "# # from util.inference import phonemes_to_token_ids\n",
    "# import torch\n",
    "\n",
    "# def text_to_token_ids(text_seq, config_path='/home/chengxin/chengxin/Dataset_Sound/MetaData/vaflow2_meta/meta/token_list.json'):\n",
    "#     tokenizer = PhonemeTokenizer(g2p_type=\"g2p_en_no_space\", non_linguistic_symbols=None,)\n",
    "#     with open(config_path, 'r') as f:\n",
    "#         phoneme2id = yaml.safe_load(f)\n",
    "#     token_ids = []\n",
    "\n",
    "#     for text in text_seq:\n",
    "#         phonemes = [\"<blank>\"] + tokenizer.text2tokens(text) + [\"<blank>\"]\n",
    "#         token_id = [phoneme2id.get(p, phoneme2id.get('<blank>', 0)) for p in phonemes]\n",
    "#         token_ids.append(torch.tensor(token_id, dtype=torch.long))\n",
    "#     return token_ids\n",
    "\n",
    "# text_seq = [\"I will give a try. Believe in me.\", \"Fuck me\", \"And For me the surprise was\"]\n",
    "# text_to_token_ids(text_seq=text_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e507da1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os, time, random, einops, wandb, inspect\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from tqdm import tqdm\n",
    "# from PIL import Image\n",
    "\n",
    "\n",
    "# import torch, torchaudio\n",
    "# import torch.nn as nn\n",
    "# import torchvision.io as tvio\n",
    "# import torchvision.transforms as transforms\n",
    "# from torchvision.transforms import InterpolationMode\n",
    "\n",
    "\n",
    "# import yaml\n",
    "# import decord\n",
    "# from worker.vaflow_sda_dit_noise_text_mel import VAFlow\n",
    "# from worker.dp_tts import DurationPredictor\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "# def maxPathSumWithPath(matrix):\n",
    "#     m, n = len(matrix), len(matrix[0])\n",
    "#     dp = [[float('-inf')] * n for _ in range(m)]\n",
    "#     prev = [[None] * n for _ in range(m)]  # 记录前驱方向\n",
    "    \n",
    "#     dp[0][0] = matrix[0][0]\n",
    "#     for j in range(1, n):\n",
    "#         dp[0][j] = dp[0][j-1] + matrix[0][j]\n",
    "#         prev[0][j] = 'left'\n",
    "#     for i in range(1, m):\n",
    "#         dp[i][0] = dp[i-1][0] + matrix[i][0]\n",
    "#         prev[i][0] = 'up'\n",
    "#     for i in range(1, m):\n",
    "#         for j in range(1, n):\n",
    "#             max_val = float('-inf')\n",
    "#             direction = None\n",
    "            \n",
    "#             if i > 0 and dp[i-1][j] > max_val:\n",
    "#                 max_val = dp[i-1][j]\n",
    "#                 direction = 'up'\n",
    "#             if j > 0 and dp[i][j-1] > max_val:\n",
    "#                 max_val = dp[i][j-1]\n",
    "#                 direction = 'left'\n",
    "#             if i > 0 and j > 0 and dp[i-1][j-1] > max_val:\n",
    "#                 max_val = dp[i-1][j-1]\n",
    "#                 direction = 'diag'\n",
    "            \n",
    "#             dp[i][j] = max_val + matrix[i][j]\n",
    "#             prev[i][j] = direction\n",
    "    \n",
    "\n",
    "#     path = []\n",
    "#     i, j = m-1, n-1\n",
    "#     while i >= 0 and j >= 0:\n",
    "#         path.append((i, j))\n",
    "#         if prev[i][j] == 'up':\n",
    "#             i -= 1\n",
    "#         elif prev[i][j] == 'left':\n",
    "#             j -= 1\n",
    "#         elif prev[i][j] == 'diag':\n",
    "#             i -= 1\n",
    "#             j -= 1\n",
    "#         else:\n",
    "#             break \n",
    "    \n",
    "#     path.reverse()  \n",
    "#     return dp[m-1][n-1], path\n",
    "\n",
    "\n",
    "\n",
    "# def get_video_frames(\n",
    "#     va_path,\n",
    "#     video_process_config={'duration':10.0, 'resize_input_size': [224, 224], 'target_sampling_rate': 10, 'raw_duration_min_threshold':0.05},\n",
    "#     backend=\"decord\"\n",
    "# ):\n",
    "#     def check_and_drop(int_list, min_value, max_value):\n",
    "#         return [x for x in int_list if min_value <= x <= max_value]\n",
    "#     video_fram_h, video_frame_w = video_process_config['resize_input_size']\n",
    "#     clip_transform = transforms.Compose([\n",
    "#             transforms.Resize((video_fram_h, video_frame_w), interpolation=InterpolationMode.BICUBIC),\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "#     ])\n",
    "#     video_process_config['target_frame_length'] = int(video_process_config['duration'] * video_process_config['target_sampling_rate'])\n",
    "\n",
    "#     if backend == \"torchvision\":\n",
    "#         try:\n",
    "#             frame_data, _, meta = tvio.read_video(va_path, pts_unit=\"sec\", output_format=\"THWC\")\n",
    "#             video_raw_fps = meta[\"video_fps\"]\n",
    "#         except BaseException as e:\n",
    "#             raise e\n",
    "#         video_raw_frame_num = len(frame_data)\n",
    "#         video_raw_duration = video_raw_frame_num / video_raw_fps\n",
    "#         if video_raw_duration <= video_process_config['raw_duration_min_threshold']:\n",
    "#             raise RuntimeError(f\"Video duration {video_raw_duration} is too short, less than {video_process_config['raw_duration_min_threshold']}.\")\n",
    "#         video_sampled_frame_ids = [\n",
    "#             round(i * video_raw_fps / video_process_config['target_sampling_rate'])\n",
    "#             for i in range(video_process_config['target_frame_length'])\n",
    "#         ]\n",
    "#         video_sampled_frame_ids = check_and_drop(video_sampled_frame_ids, 0, video_raw_frame_num-1)\n",
    "#         sampled_frames = frame_data[video_sampled_frame_ids, :, :, :]\n",
    "#         if sampled_frames.dtype != torch.uint8:\n",
    "#             if sampled_frames.max() <= 1.0:\n",
    "#                 sampled_frames = (sampled_frames * 255).byte()\n",
    "#             else:\n",
    "#                 sampled_frames = sampled_frames.byte()\n",
    "#         sampled_frames = [Image.fromarray(frame.numpy()) for frame in sampled_frames]\n",
    "#     elif backend == \"decord\":\n",
    "#         try:\n",
    "#             decord_vr = decord.VideoReader(va_path, num_threads=1, ctx=decord.cpu(0))\n",
    "#         except BaseException as e:\n",
    "#             raise e\n",
    "#         video_raw_frame_num = len(decord_vr)\n",
    "#         video_raw_fps = decord_vr.get_avg_fps()\n",
    "#         video_raw_duration = video_raw_frame_num / video_raw_fps\n",
    "#         if video_raw_duration <= video_process_config['raw_duration_min_threshold']:\n",
    "#             raise RuntimeError(f\"Video duration {video_raw_duration} is too short, less than {video_process_config['raw_duration_min_threshold']}.\")\n",
    "            \n",
    "#         # Sample frames with target sampling rate, \n",
    "#         # NOTE Implemention of `FPS`` here is just selecting frames in raw 30fps video frames\n",
    "#         video_sampled_frame_ids = [ round(i * video_raw_fps / video_process_config['target_sampling_rate'])\n",
    "#                                     for i in range(video_process_config['target_frame_length']) ]\n",
    "#         video_sampled_frame_ids = check_and_drop(video_sampled_frame_ids, 0, video_raw_frame_num-1)\n",
    "#         sampled_frames = decord_vr.get_batch(video_sampled_frame_ids).asnumpy()\n",
    "            \n",
    "#         if sampled_frames.dtype != np.uint8:\n",
    "#             if sampled_frames.max() <= 1.0:\n",
    "#                 sampled_frames = (sampled_frames * 255).astype(np.uint8)\n",
    "#             else:\n",
    "#                 sampled_frames = sampled_frames.astype(np.uint8)\n",
    "            \n",
    "#         sampled_frames = [Image.fromarray(frame) for frame in sampled_frames]\n",
    "\n",
    "#     else:\n",
    "#         raise NotImplementedError(f\"Backend `{backend}` is not implemented.\")\n",
    "\n",
    "#     ''' Process frames '''\n",
    "#     p = video_process_config['target_frame_length'] - len(sampled_frames)\n",
    "#     if p > 0:\n",
    "#         w, h = sampled_frames[0].size  \n",
    "#         blank_pil = Image.new('RGB', (h, w), (0, 0, 0))\n",
    "#         sampled_frames = sampled_frames + [blank_pil] * p\n",
    "#     else:\n",
    "#         sampled_frames = sampled_frames[0 : video_process_config['target_frame_length']]\n",
    "#     assert len(sampled_frames) == video_process_config['target_frame_length'], f\"Sampled frames length {len(sampled_frames)} is not equal to target length {self.video_target_frame_len}.\"\n",
    "#     # Transform and stack\n",
    "#     sampled_frames = [clip_transform(sampled_frame) for sampled_frame in sampled_frames ]\n",
    "#     processed_frames = torch.stack(sampled_frames, dim=0)\n",
    "#     return processed_frames\n",
    "\n",
    "\n",
    "\n",
    "# def phonemes_to_token_ids(phoneme_seq, config_path='/home/chengxin/chengxin/Dataset_Sound/MetaData/vaflow2_meta/meta/token_list.json'):\n",
    "#     with open(config_path, 'r') as f:\n",
    "#         phoneme2id = yaml.safe_load(f)\n",
    "#     token_ids = [phoneme2id.get(p, phoneme2id.get('<unk>', 0)) for p in phoneme_seq]\n",
    "#     return torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "\n",
    "\n",
    "# def pad_sequence(sequences, padding_value=0, max_length=None, pad_type='right', target_dim=None):\n",
    "#     if not sequences:\n",
    "#         raise ValueError(\"sequences should not be empty\")\n",
    "\n",
    "#     # Determine which dimension to pad (default: 0)\n",
    "#     if target_dim is None:\n",
    "#         target_dim = 0\n",
    "\n",
    "#     # Find max length\n",
    "#     lengths = [s.shape[target_dim] for s in sequences]\n",
    "#     pad_len = max(lengths) if max_length is None else max_length\n",
    "\n",
    "\n",
    "#     # Pad each tensor\n",
    "#     padded = []\n",
    "#     for s in sequences:\n",
    "#         pad_size = pad_len - s.shape[target_dim]\n",
    "#         if pad_size < 0:\n",
    "#             raise ValueError(\"Some sequence is longer than pad_len\")\n",
    "#         pad_shape = [0] * (2 * s.dim())\n",
    "#         pad_shape[2 * (s.dim() - target_dim) - 1] = pad_size  # pad right\n",
    "#         if pad_type == 'left':\n",
    "#             pad_shape[2 * (s.dim() - target_dim) - 2] = pad_size  # pad left\n",
    "#             pad_shape[2 * (s.dim() - target_dim) - 1] = 0\n",
    "#         s_padded = torch.nn.functional.pad(s, pad_shape, mode='constant', value=padding_value)\n",
    "#         padded.append(s_padded)\n",
    "#     return torch.stack(padded, dim=0)\n",
    "\n",
    "\n",
    "\n",
    "# def adjust_speech(speech_start_duration, phone_id, duration_matrix, device):\n",
    "#     # phone_id [phone_length]    duration_matrix [250, phone_length]\n",
    "#     # phone_id = torch.cat((torch.tensor([59]).to(phone_id), phone_id))\n",
    "#     if speech_start_duration == 0:\n",
    "#         return torch.cat((torch.tensor([0], dtype=torch.int).to(phone_id), phone_id)), \\\n",
    "#                 torch.cat([duration_matrix, torch.zeros([250 ,1]).to(duration_matrix)], dim = -1)\n",
    "    \n",
    "    \n",
    "#     assert phone_id[0] == 59\n",
    "#     phone_id = torch.cat((torch.tensor([0], dtype=torch.int).to(phone_id), phone_id))            # [1 + phone_length]\n",
    "#     silence_length = int(round(speech_start_duration*25, 0))\n",
    "#     duration_matrix_new = torch.zeros([silence_length, phone_id.shape[-1]], device=device)     # [append_dur_length, 1 + phone_length]\n",
    "#     duration_matrix_new[:, 0] = 1\n",
    "\n",
    "#     assert duration_matrix[-silence_length].sum() == 0\n",
    "#     duration_matrix = duration_matrix[:-silence_length]                              # [250 - append_dur_length, phone_length]\n",
    "#     duration_matrix = F.pad(duration_matrix, (1, 0), mode='constant', value=0)       # [250 - append_dur_length, 1 + phone_length]\n",
    "#     duration_matrix_new = torch.cat((duration_matrix_new, duration_matrix), dim=0)   # [250, 1 + phone_length]\n",
    "#     return phone_id, duration_matrix_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fe7a74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "\n",
    "# def phonemes_to_token_ids(phoneme_seq, config_path='/home/chengxin/chengxin/Dataset_Sound/MetaData/vaflow2_meta/meta/token_list.json'):\n",
    "#     with open(config_path, 'r') as f:\n",
    "#         phoneme2id = yaml.safe_load(f)\n",
    "#     token_ids = [phoneme2id.get(p, phoneme2id.get('<unk>', 0)) for p in phoneme_seq]\n",
    "#     return torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "\n",
    "# def pad_sequence(sequences, padding_value=0, max_length=None, pad_type='right', target_dim=None):\n",
    "#     if not sequences:\n",
    "#         raise ValueError(\"sequences should not be empty\")\n",
    "\n",
    "#     # Determine which dimension to pad (default: 0)\n",
    "#     if target_dim is None:\n",
    "#         target_dim = 0\n",
    "\n",
    "#     # Find max length\n",
    "#     lengths = [s.shape[target_dim] for s in sequences]\n",
    "#     pad_len = max(lengths) if max_length is None else max_length\n",
    "\n",
    "\n",
    "#     # Pad each tensor\n",
    "#     padded = []\n",
    "#     for s in sequences:\n",
    "#         pad_size = pad_len - s.shape[target_dim]\n",
    "#         if pad_size < 0:\n",
    "#             raise ValueError(\"Some sequence is longer than pad_len\")\n",
    "#         pad_shape = [0] * (2 * s.dim())\n",
    "#         pad_shape[2 * (s.dim() - target_dim) - 1] = pad_size  # pad right\n",
    "#         if pad_type == 'left':\n",
    "#             pad_shape[2 * (s.dim() - target_dim) - 2] = pad_size  # pad left\n",
    "#             pad_shape[2 * (s.dim() - target_dim) - 1] = 0\n",
    "#         s_padded = torch.nn.functional.pad(s, pad_shape, mode='constant', value=padding_value)\n",
    "#         padded.append(s_padded)\n",
    "#     return torch.stack(padded, dim=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # phoneme_seqs = ['<blank> S OW1 L EH1 T S T AO1 K AH0 B AW1 T AH0 B Z AO1 R P SH AH0 N AH1 V L AY1 T <blank>', '<blank> S OW1 L EH1 T S T AO1 K AH0 B AW1 T AH0 B Z AO1 R P SH AH0 N AH1 V T <blank>']\n",
    "# # phoneme_seqs = [phoneme_seq.split(\" \") for phoneme_seq in phoneme_seqs]\n",
    "# # avhubert_feature_paths = None\n",
    "# # speech_durations=[1.6, 1.6]\n",
    "# # device=\"cuda:0\"\n",
    "\n",
    "# # # Convert phoneme sequences to token ids and pad\n",
    "# # token_ids = [phonemes_to_token_ids(seq) for seq in phoneme_seqs]\n",
    "# # phone_id = pad_sequence(token_ids, padding_value=0).to(device)  # [B, max_seq_len]\n",
    "# # print(phone_id.shape)\n",
    "# # phone_length = torch.tensor([len(seq) for seq in token_ids], dtype=torch.long, device=device)  # [B]\n",
    "\n",
    "# # # Prepare avhubert features and pad\n",
    "# # avhubert_list = []\n",
    "# # avhubert_length = []\n",
    "# # if avhubert_feature_paths is not None:\n",
    "# #     for path in avhubert_feature_paths:\n",
    "# #         feat = torch.tensor(np.load(path), device=device)\n",
    "# #         avhubert_list.append(feat)\n",
    "# #         avhubert_length.append(feat.shape[0])\n",
    "# # else:\n",
    "# #     assert speech_durations is not None\n",
    "# #     for dur in speech_durations:\n",
    "# #         feat = torch.ones(int(25 * dur), 1024, device=device)\n",
    "# #         avhubert_list.append(feat)\n",
    "# #         avhubert_length.append(feat.shape[0])\n",
    "# # avhubert = pad_sequence(avhubert_list, padding_value=0.0, max_length = 250)  # [B, max_avhubert_len, 1024]\n",
    "# # avhubert_length = torch.tensor(avhubert_length, dtype=torch.long, device=device)  # [B]\n",
    "\n",
    "# # batch = {\n",
    "# #     \"video_id\": None,\n",
    "# #     \"duration_matrix\": None,\n",
    "# #     \"duration_span\": None,\n",
    "# #     \"avhubert\": avhubert,\n",
    "# #     \"avhubert_length\": avhubert_length,\n",
    "# #     \"phone_id\": phone_id,\n",
    "# #     \"phone_length\": phone_length,\n",
    "# # }\n",
    "\n",
    "\n",
    "# # attns = dp.predict_step(batch)\n",
    "# # attns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d0ebd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# va_paths = ['/home/chengxin/chengxin/Dataset_Sound/LRS2/video_25fps/6330311066473698535_27418/6330311066473698535_27418_00000.mp4']\n",
    "# # va_paths = ['/home/chengxin/chengxin/Dataset_Sound/VGGSound/dataset/test/10/__2MwJ2uHu0_000004.mp4']\n",
    "# text_seq = ['And, For me ,the surprise was']\n",
    "# speech_start_durations = None # [2]   \n",
    "# video_process_config = {'duration':10.0, 'resize_input_size': [224, 224], 'target_sampling_rate': 10, 'raw_duration_min_threshold':0.05}\n",
    "\n",
    "# # 可以没有。speech_durations和avhubert_feature_paths只需要有一个\n",
    "# # mix generation的效果还没测\n",
    "# speech_durations = [2.5]\n",
    "# avhubert_feature_paths = ['/nfs-04/yuyue/visualtts_datasets/lrs2/lrs2_for_espnet/preprocessed_data/lip_feature/6330311066473698535_27418_00000.npy']\n",
    "# ref_audio_ebd_paths = ['/home/chengxin/chengxin/Dataset_Sound/LRS2/speaker_emb/6330311066473698535_27418/6330311066473698535_27418_00000.npy']\n",
    "\n",
    "\n",
    "\n",
    "# infer_videos(\n",
    "#     vaflow_model = vaflow, \n",
    "#     dp_model = dp,\n",
    "#     save_path='/home/chengxin/chengxin/vasflow/test', \n",
    "#     mp4_paths=va_paths, \n",
    "#     video_process_config = video_process_config, \n",
    "#     text_seq = text_seq,                              \n",
    "#     avhubert_feature_paths = avhubert_feature_paths,  # 抽取avhubert_feature的代码没有实现\n",
    "#     speech_durations=speech_durations,\n",
    "#     speech_start_durations=speech_start_durations,\n",
    "#     ref_audio_ebd_paths = ref_audio_ebd_paths,        # 抽取ref_audio_ebd的代码没有实现\n",
    "#     device=device\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60f1a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3e81bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15263 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15263/15263 [02:35<00:00, 98.43it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/chengxin/chengxin/dataset/VGGSound/generated_audios/vasflow/10/dLLRg0sur6g_000060.wav'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "for file in tqdm(glob('/home/chengxin/chengxin/vasflow/log/2025_11_03-13_*-vaflow_sda_dit_noise_text_mel_infer_vgg/predict/video/*.wav')):\n",
    "    name = file.split('/')[-1].replace('_00.wav', '.wav').replace('audio_', '')\n",
    "    target_file = f\"/home/chengxin/chengxin/dataset/VGGSound/generated_audios/vasflow/10/{name}\"\n",
    "    shutil.copy(file, target_file)\n",
    "target_file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vagen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
